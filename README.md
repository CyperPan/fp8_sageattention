# fp8_sageattention
GPU: NVIDIA H100 80GB HBM3
PyTorch: 2.9.0+cu128
Triton: 3.5.0

============================================================
方法一：triton.testing.do_bench
============================================================
  B=1, H=32, Seq=1024, D=128  |  0.018 ms  |  1421.0 GB/s
  B=1, H=32, Seq=4096, D=128  |  0.044 ms  |  2278.7 GB/s
  B=1, H=32, Seq=8192, D=128  |  0.077 ms  |  2614.9 GB/s
  B=2, H=32, Seq=4096, D=128  |  0.077 ms  |  2606.9 GB/s
  B=1, H=32, Seq=16384, D=128  |  0.142 ms  |  2833.6 GB/s

============================================================
方法二：CUDA Events
============================================================
  Config: B=1, H=32, Seq=4096, D=128
  Average: 0.047 ms  (100 iterations)

============================================================
方法四：FP8 vs INT8 对比
============================================================
  Config                              |  INT8 (ms) |   FP8 (ms) |  Speedup
---------------------------------------------------------------------------
  B=1, H=32, Seq=1024, D=128          |      0.020 |      0.018 |    1.10x
  B=1, H=32, Seq=4096, D=128          |      0.050 |      0.044 |    1.11x
  B=1, H=32, Seq=8192, D=128          |      0.084 |      0.077 |    1.09x
  B=1, H=32, Seq=16384, D=128         |      0.152 |      0.142 |    1.07x
  B=1, H=32, Seq=32768, D=128         |      0.290 |      0.272 |    1.06x
